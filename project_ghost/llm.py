import json
import os
import yaml
from openai import OpenAI
from pprint import pformat
import json, re

# ==== 文件路径设置 ====
VULN_OUTPUT_PATH = "/home/xuehuanhuan/source-sink-ghidra/project_ghost/GhOST Output/bbb-250809_233749/vuln_output.json"
CONFIG_PATH = "/home/xuehuanhuan/source-sink-ghidra/project_ghost/config.yaml"
RESULT_PATH = "/home/xuehuanhuan/source-sink-ghidra/project_ghost/GhOST Output/bbb-250809_233749/vuln_analysis_results.json"

# ==== 模板字符串 ====
SYSTEM_TEMPLATE = (
    "You are a static-analysis assistant for LATTE-style, function-by-function inspection.\n"
    "GOAL: For the CURRENT function only, summarize (a) which parameters are tainted and their size/range features, "
    "(b) what calls are made and which callee args are tainted (with sizes/ranges), (c) any new sources, (d) sinks hit.\n"
    "OUTPUT: Return ONE single-line JSON object ONLY (no Markdown/prose beyond `note`).\n"
    'Use EXACTLY these keys (keep arrays minimal; omit unknown fields by using "unk"):\n'
    "{"
    '"fn":"string", // func to be analysed '
    '"sources":[{"name":"string","out":"var","label":"S#"}],// taint source in this func'
    '"calls":[{"callee":"string","args":[{"idx":int,"from_param":int|"local","array_size":"str|unk","int_range":"str|unk"}]}],//function call in this func'
    '"next_fns":["string"],//next func will be analyzed'
    '"note":"brief natural hint if truly useful"'
    '"sink":["string"] //sink function name,assigned by prompt'
    "}\n"
    "RULES:\n"
    "- Report only facts from this snippet or explicit context; no code restatement.\n"
    "- Keep param features relevant to overflow (size, array len, int range).\n"
    "- Map caller params via `from_param` index or 'local'.\n"
    "- Record new sources; propagate taint only if visible.\n"
    "- Use [] or 'unk' if none; no explanations.\n"
    "- Ignore misleading names; reason from usage.\n"
    "\nINHERIT RULE:\n"
    "- If previous assistant JSON shows a call to this function, copy arg {taint,size,range,label} to params[i] unless current code overrides.\n"
)


START_TEMPLATE = (
    "As a program analyst, I give you snippets of C code generated by decompilation, "
    "using '{source}' as the taint source, '{sink}' as the sink, and the parameter that may be marked as the taint label to extract the taint data flow."
    "Pay attention to the data alias and tainted data operations. Output in the form of data flows:\n{code}"
)

SSMIDDLE_TEMPLATE = (
    "Continue to analyze function according to the above taint analysis results. Pay attention to the data alias marked as the taint label."
    "and tainted data operations. Important! Refer to the analysis results of the upstream function to infer possible values for the current parameters."
    'if you find the sink func,analyse function calls and save into the "calls" iteM:\n{code}'
)
MIDDLE_TEMPLATE = (
    "Continue to analyze function according to the above taint analysis results. "
    "Pay attention to the data alias marked as the taint label and tainted data operations. "
    "Important! Refer to the analysis results of the upstream function to infer possible values for the current parameters. "
    "For EVERY function call (including sink functions like strcpy, memcpy, etc.), "
    "record it in the 'calls' array with each argument's taint,array_size,int_range,label;a call may have more than one arg(strcpy have two args,etc.)"
    "\n{code}"
)

END_TEMPLATE = (
    "{code}\n"
    "Based on the func_call with above parameters, determine whether the code has CWE-120 buffer overflow vulnerabilities. "
    "Respond with 'Yes' or 'No' as the first word of your answer, to support automated accuracy analysis. "
    "Important! Analyze any bound applied to the arguments of sink calls that appear in the last function's analysis results. "
    "Use only information explicitly present in the last function's analysis results; do not assume or invent additional facts. "
    "That is, the size identified in this analysis is the exact and only possible runtime size for this case. "
    "There are no other possible values in any context, past, present, or future. "
    "Do not speculate or consider hypothetical scenarios outside of the given data. "
    "If the size of the source argument is greater than or equal to the size of the destination argument, output 'Yes'. Otherwise, output 'No'. "
    "Provide your explanation."
)
END_TEMPLATE = (
    "{code}\n"
    "Task: Decide whether this call is a CWE-120 buffer overflow.\n"
    "Frozen world: The given argument sizes are exact and the only possible values. "
    "Do NOT speculate or consider any other contexts or inputs.\n"
    "Decision rule: If source_size > destination_size OR source_size == destination_size → answer 'Yes'. "
    "If source_size < destination_size → answer 'No'. "
    "If any required size is unknown → answer 'Yes'.\n"
    "give your explain"
)


# ==== 载入配置 ====
if not os.path.isfile(CONFIG_PATH):
    raise FileNotFoundError(f"Missing config: {CONFIG_PATH}")

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    cfg = yaml.safe_load(f)

api_key = cfg.get("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEY missing in config.yaml")

api_base = cfg.get("API_BASE", "https://api.openai.com/v1")
model = cfg.get("MODEL", "gpt-4o-mini")
available_models = cfg.get("AVAILABLE_MODELS", [])

if model not in available_models:
    raise ValueError(f"Selected model '{model}' not in AVAILABLE_MODELS: {available_models}")

# ==== 初始化 OpenAI 客户端 ====
client = OpenAI(api_key=api_key, base_url=api_base)


# ==== 分析函数流 ====
"""def analyze_flow(flow: dict) -> str:
    source = flow.get("source")
    sink = flow.get("sink")
    func_keys = sorted(int(k) for k in flow if k.isdigit())
    funcs = [flow[str(i)] for i in func_keys]

    messages = [
        {"role": "system", "content": "You are a security auditing assistant. Help identify potential vulnerabilities in binary code."},
        {"role": "user", "content": START_TEMPLATE.format(source=sink, code=funcs[0])},
    ]
    for code in funcs[1:]:
        messages.append({"role": "user", "content": MIDDLE_TEMPLATE.format(code=code)})
    messages.append({"role": "user", "content": END_TEMPLATE})
    print(messages)
    # 发送请求
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.5,
    )
    return response.choices[0].message.content"""


def build_call_context_for(messages, target_fn: str) -> str:
    """
    从上一轮 assistant 的 JSON 中，抽取 calls 里指向 target_fn 的 args，
    生成 [CALL_CONTEXT] 文本块；若没有可用信息，返回空串。
    """

    def parse_assistant_json(text: str):
        """尽量把上一轮assistant输出解析为JSON（容错：返回None不报错）。"""
        text = text.strip()
        # 常见情况就是严格的一行JSON
        try:
            return json.loads(text)
        except Exception:
            pass
        # 退而求其次：抓第一个以{开头、}结尾的片段
        try:
            start = text.find("{")
            end = text.rfind("}")
            if start != -1 and end != -1 and end > start:
                return json.loads(text[start : end + 1])
        except Exception:
            return None
        return None

    # 找最近一条assistant消息
    for msg in reversed(messages):
        if msg.get("role") == "assistant":
            data = parse_assistant_json(msg.get("content", ""))
            if not isinstance(data, dict):
                continue
            calls = data.get("calls") or []
            for call in calls:
                if str(call.get("callee")) == target_fn:
                    args = call.get("args") or []
                    # 只保留我们关心的字段，缺失用 "unk"
                    slim_args = []
                    for a in args:
                        slim_args.append(
                            {
                                "idx": a.get("idx", 0),
                                "array_size": str(a.get("array_size", "unk")),
                                "int_range": str(a.get("int_range", "unk")),
                                "label": str(a.get("label", "unk")),
                            }
                        )
                    ctx = "[CALL_CONTEXT]\n" f"callee={target_fn}\n" "arg_hints: " + json.dumps(slim_args, separators=(",", ":")) + "\n"
                    return ctx
            break  # 最近一条assistant里没有命中就不再更早找
    return ""  # 没有可用的上游信息


FUNC_NAME_RE = re.compile(r"\b([A-Za-z_]\w*)\s*\(")


def extract_fn_name(code: str) -> str:
    """从C反编译片段里提取函数名（取第一个匹配）。"""
    m = FUNC_NAME_RE.search(code)
    return m.group(1) if m else ""


def analyze_flow(flow: dict) -> str:
    source = flow.get("source")
    sink = flow.get("sink")
    func_keys = sorted(int(k) for k in flow if k.isdigit())
    funcs = [flow[str(i)] for i in func_keys]

    # 初始化对话
    messages = [
        {"role": "system", "content": SYSTEM_TEMPLATE},
    ]

    # 首轮：发送 START_TEMPLATE
    start_msg = START_TEMPLATE.format(source=source, sink=sink, code=funcs[0])
    # start_msg = RSTART_TEMPLATE.format(sink=sink, code=funcs[-1])
    messages.append({"role": "user", "content": start_msg})
    print(messages)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
    )
    messages.append({"role": "assistant", "content": response.choices[0].message.content})

    # 中间轮：依次发送每个 MIDDLE_TEMPLATE
    for code in funcs[1:]:
        target_fn = extract_fn_name(code)
        call_ctx = build_call_context_for(messages, target_fn)
        # for code in reversed(funcs[:-1]):
        middle_msg = MIDDLE_TEMPLATE.format(code=(call_ctx + code))
        # middle_msg = RMIDDLE_TEMPLATE.format(code=code)
        messages.append({"role": "user", "content": middle_msg})
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
        )
        messages.append({"role": "assistant", "content": response.choices[0].message.content})

    # 最后一轮：发送 END_TEMPLATE
    # end_msg = REND_TEMPLATE.format(source=source)
    call_ctx = build_call_context_for(messages, sink)
    aa = END_TEMPLATE.format(code=call_ctx)
    messages.append({"role": "user", "content": aa})
    # messages.append({"role": "user", "content": end_msg})
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
    )
    final_reply = response.choices[0].message.content
    messages.append({"role": "assistant", "content": final_reply})
    formatted = pformat(messages, width=100)
    print(formatted.replace("\\n", "\n"))
    # ✅ 返回最后的回复（不返回中间的内容）
    return final_reply


# ==== 主程序 ====
def main():
    if not os.path.isfile(VULN_OUTPUT_PATH):
        raise FileNotFoundError(f"Missing input file: {VULN_OUTPUT_PATH}")

    with open(VULN_OUTPUT_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    results = {}
    for vid, flow in data.items():
        print(f"Analyzing {vid} with model {model} (API base: {api_base})...")
        # try:
        result = analyze_flow(flow)
        # except Exception as e:
        #    result = f"Error during analysis: {e}"
        results[vid] = result
        print(f"--- {vid} Result ---\n{result}\n")

    with open(RESULT_PATH, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"Analysis complete. Output written to {RESULT_PATH}")


if __name__ == "__main__":
    main()
